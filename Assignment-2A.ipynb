{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2A\n",
    "\n",
    "Name: Taaha Waseem <br>\n",
    "ID: 28888286 <br>\n",
    "Last Updated: 27/04/2021 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 Creating SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-90a539942969>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Import SparkConf class into program\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# Import SparkContext and SparkSession classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m \u001b[0;31m# Spark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m \u001b[0;31m# Spark SQL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "# Import SparkConf class into program\n",
    "from pyspark import SparkConf\n",
    "# Import SparkContext and SparkSession classes\n",
    "from pyspark import SparkContext # Spark\n",
    "from pyspark.sql import SparkSession # Spark SQL\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "%matplotlib inline\n",
    "import pandas as pd # Pandas\n",
    "\n",
    "import datetime\n",
    "\n",
    "master = \"local[*]\"\n",
    "app_name = \"Assignment 2A\"\n",
    "# Setup configuration parameters for Spark\n",
    "spark_conf = SparkConf().setMaster(master).setAppName(app_name)\n",
    "\n",
    "# Creating SparkSession\n",
    "spark = SparkSession.builder.config(conf=spark_conf)\\\n",
    "                            .config('spark.sql.session.timeZone', 'Australia/Melbourne')\\\n",
    "                            .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2 Defining Data Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, IntegerType, DateType, StringType, TimestampType, ArrayType, BooleanType\n",
    "from pyspark.sql.types import StructField\n",
    "import datetime\n",
    "#Reference: https://stackoverflow.com/questions/48698062/set-schema-in-pyspark-dataframe-read-csv-with-null-elements\n",
    "\n",
    "\n",
    "# Defining schema for pedestrian count\n",
    "pedSchema = StructType([\n",
    "    StructField(\"ID\", IntegerType()),\n",
    "    StructField(\"Date_Time\", StringType(), True),\n",
    "    StructField(\"Year\", IntegerType()),\n",
    "    StructField(\"Month\", StringType()),\n",
    "    StructField(\"Mdate\", IntegerType()),\n",
    "    StructField(\"Day\", StringType()),\n",
    "    StructField(\"Time\", IntegerType()),\n",
    "    StructField(\"Sensor_ID\", IntegerType()),\n",
    "    StructField(\"Sensor_Name\", StringType()),\n",
    "    StructField(\"Hourly_Counts\", IntegerType())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3 Loading the pedestrian count csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- Date_Time: timestamp (nullable = true)\n",
      " |-- Year: integer (nullable = true)\n",
      " |-- Month: string (nullable = true)\n",
      " |-- Mdate: integer (nullable = true)\n",
      " |-- Day: string (nullable = true)\n",
      " |-- Time: integer (nullable = true)\n",
      " |-- Sensor_ID: integer (nullable = true)\n",
      " |-- Sensor_Name: string (nullable = true)\n",
      " |-- Hourly_Counts: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
    "\n",
    "# Loading the pedestrian count csv using the schema defined earlier\n",
    "ped_df = spark.read.format('csv').\\\n",
    "    option('header', 'true').\\\n",
    "    schema(pedSchema).\\\n",
    "    load('Pedestrian_Counting_System_-_Monthly__counts_per_hour.csv')\n",
    "\n",
    "# Creating function to transform Date_Time column to TimeStampType\n",
    "func = F.udf(lambda x: datetime.datetime.strptime(x, '%m/%d/%Y %I:%M:%S %p'), TimestampType())\n",
    "# The function is implemented on the Date_Time column\n",
    "ped_df = ped_df.withColumn('Date_Time', func(F.col('Date_Time')))\n",
    "\n",
    "ped_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.4 Adding above_threshold column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----+--------+-----+------+----+---------+----------------------------+-------------+---------------+\n",
      "|ID     |Date_Time          |Year|Month   |Mdate|Day   |Time|Sensor_ID|Sensor_Name                 |Hourly_Counts|above_threshold|\n",
      "+-------+-------------------+----+--------+-----+------+----+---------+----------------------------+-------------+---------------+\n",
      "|2887628|2019-11-01 17:00:00|2019|November|1    |Friday|17  |34       |Flinders St-Spark La        |300          |0              |\n",
      "|2887629|2019-11-01 17:00:00|2019|November|1    |Friday|17  |39       |Alfred Place                |604          |0              |\n",
      "|2887630|2019-11-01 17:00:00|2019|November|1    |Friday|17  |37       |Lygon St (East)             |216          |0              |\n",
      "|2887631|2019-11-01 17:00:00|2019|November|1    |Friday|17  |40       |Lonsdale St-Spring St (West)|627          |0              |\n",
      "|2887632|2019-11-01 17:00:00|2019|November|1    |Friday|17  |36       |Queen St (West)             |774          |0              |\n",
      "+-------+-------------------+----+--------+-----+------+----+---------+----------------------------+-------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Function that returns 1 if hourly counts > 2000, else 0\n",
    "func = F.udf(lambda x: 1 if x >= 2000 else 0, IntegerType())\n",
    "\n",
    "# Applying the function on the dataframe to add another column\n",
    "ped_df = ped_df.withColumn('above_threshold', func(F.col('Hourly_Counts')))\n",
    "\n",
    "# Displaying first 5 records\n",
    "ped_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Exploring the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 Basic Stastics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+------------------+------------------+------------------+------------------+-----------------+\n",
      "|summary|               ID|              Year|             Mdate|              Time|         Sensor_ID|    Hourly_Counts|\n",
      "+-------+-----------------+------------------+------------------+------------------+------------------+-----------------+\n",
      "|  count|          3435106|           3435106|           3435106|           3435106|           3435106|          3435106|\n",
      "|   mean|        1717553.5|2016.0032330880038|15.751918863639142|11.459955238644746|22.978422791028866|560.7805942524044|\n",
      "| stddev|991629.8312350252|3.1237869143646275|  8.79918757461428| 6.943473866829414|16.229792156265397|809.9942576353371|\n",
      "|    min|                1|              2009|                 1|                 0|                 1|                0|\n",
      "|    25%|           858830|              2014|                 8|                 5|                 9|               50|\n",
      "|    50%|          1717623|              2016|                16|                11|                19|              210|\n",
      "|    75%|          2576510|              2019|                23|                17|                34|              722|\n",
      "|    max|          3435106|              2020|                31|                23|                71|            15979|\n",
      "+-------+-----------------+------------------+------------------+------------------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Displaying basic summary and statistics\n",
    "ped_df.select('ID', 'Year', 'Mdate', 'Time', 'Sensor_ID', 'Hourly_Counts').summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 Count of above-threshold and below-threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of above threshold records: 250942\n",
      "Total number of below threshold records: 3184164\n"
     ]
    }
   ],
   "source": [
    "# Counting rows above threshold\n",
    "aboveCount = ped_df.filter(F.col('above_threshold') == 1).count()\n",
    "# Counting rows below threshold\n",
    "belowCount = ped_df.filter(F.col('above_threshold') == 0).count()\n",
    "\n",
    "# Printing count of rows above threshold\n",
    "print(\"Total number of above threshold records: \" + str(aboveCount))\n",
    "# Printing count of rows below threshold\n",
    "print(\"Total number of below threshold records: \" + str(belowCount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a class imbalence whereby the total number of records with hourly_counts below the threshold is significantly larger than the total number of records with hourly_counts above the threshold. This would affect the classification model in a negative manner whereby the class above threshold might have a lower predictive accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Extraction and ML training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Preparing Spark ML Transformers/Estimators for features, labels and models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Prepare feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Month in Integer\n",
    "func = F.udf(lambda x: x.month, IntegerType())\n",
    "ped_df = ped_df.withColumn('Month (in integer)', func(F.col('Date_Time')))\n",
    "\n",
    "# Day of Week\n",
    "func = F.udf(lambda x: x.isocalendar()[2], IntegerType())\n",
    "ped_df = ped_df.withColumn('day_of_week', func(F.col('Date_Time')))\n",
    "\n",
    "# Week of Year\n",
    "func = F.udf(lambda x: x.isocalendar()[1], IntegerType())\n",
    "ped_df = ped_df.withColumn('week_of_year', func(F.col('Date_Time')))\n",
    "\n",
    "#Filter 2014 - 2019\n",
    "ped_df = ped_df.filter((F.col('Year') >= 2014) & (F.col('Year') <= 2019))\n",
    "\n",
    "#Filter 9pm to 11pm\n",
    "ped_df = ped_df.filter((F.col('Time') >= 21) & (F.col('Time') <= 23))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Creating Transformers/Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining input and output columns\n",
    "inputCols = ['Month (in integer)', 'Mdate', 'week_of_year', 'day_of_week', 'Time', 'Sensor_ID']\n",
    "outputCol = 'above_threshold'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "\n",
    "# Defining outputCols for One Hot Encoder\n",
    "outputCols_OHE = [f'{x}_vec' for x in inputCols]\n",
    "\n",
    "# Declaring OneHotEncoder\n",
    "encoder = OneHotEncoder(inputCols=inputCols,\n",
    "                       outputCols=outputCols_OHE).setHandleInvalid('keep')\n",
    "\n",
    "# Declaring Vector Assembler\n",
    "assembler = VectorAssembler(inputCols=outputCols_OHE,\n",
    "                           outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# Declaring Logistic Regression\n",
    "lr = LogisticRegression(featuresCol='features',labelCol='above_threshold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# Declaring Decision Tree Classifier\n",
    "dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'above_threshold', maxDepth = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# Declaring Random Forest Classifier\n",
    "rf = RandomForestClassifier(labelCol='above_threshold', featuresCol='features', numTrees=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 Creating Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Creating logistic regression pipeline\n",
    "pipelineLR = Pipeline(stages=[encoder, assembler, lr])\n",
    "\n",
    "# Creating decision tree pipeline\n",
    "pipelineDT = Pipeline(stages=[encoder, assembler, dt])\n",
    "\n",
    "# Creating random forest pipeline\n",
    "pipelineRF = Pipeline(stages=[encoder, assembler, rf])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Preparing the training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating training data (from year 2014 till 2018)\n",
    "train = ped_df.filter((F.col('Year') >= 2014) & (F.col('Year') <= 2018))\n",
    "\n",
    "# Creating testing data (year  2019)\n",
    "test = ped_df.filter(F.col('Year') == 2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Training and evaluating models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.1 Training and Predicting using Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training logistic regression\n",
    "modelLR = pipelineLR.fit(train)\n",
    "\n",
    "# Making predictions on test data\n",
    "predictionsLR = modelLR.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training decision tree\n",
    "modelDT = pipelineDT.fit(train)\n",
    "\n",
    "# Making predictions on test data\n",
    "predictionsDT = modelDT.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training random forest\n",
    "modelRF = pipelineRF.fit(train)\n",
    "\n",
    "# Making predictions on test data\n",
    "predictionsRF = modelRF.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.2 Measuring classification performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function to return metrics (Precision, Accuracy and Recall)\n",
    "# Also returns output dataframe/confusion matrix\n",
    "def print_metrics(predictions):\n",
    "    output = predictions.groupBy(\"above_threshold\", \"prediction\").count()\n",
    "    TN = 0 if output.filter('prediction = 0 AND above_threshold = 0').count() == 0\\\n",
    "        else output.filter('prediction = 0 AND above_threshold=0').first()['count']\n",
    "    TP = 0 if output.filter('prediction = 1 AND above_threshold = 1').count() == 0\\\n",
    "        else output.filter('prediction = 1 AND above_threshold=1').first()['count']\n",
    "    FN = 0 if output.filter('prediction = 0 AND above_threshold = 1').count() == 0\\\n",
    "        else output.filter('prediction = 0 AND above_threshold=1').first()['count']\n",
    "    FP = 0 if output.filter('prediction = 1 AND above_threshold = 0').count() == 0\\\n",
    "        else output.filter('prediction = 1 AND above_threshold=0').first()['count']\n",
    "    \n",
    "    # If TN, TP, FN or FP is equal to 0, add a row to the final output that shows the particular row as well\n",
    "    columns = ['above_threshold', 'prediction', 'count']\n",
    "    if TN == 0:\n",
    "        newRow = spark.createDataFrame([[0, 0, 0]])\n",
    "        output = output.union(newRow)\n",
    "    if TP == 0:\n",
    "        newRow = spark.createDataFrame([[1, 1, 0]])\n",
    "        output = output.union(newRow)\n",
    "    if FN == 0:\n",
    "        newRow = spark.createDataFrame([[0, 1, 0]])\n",
    "        output = output.union(newRow)\n",
    "    if FP == 0:\n",
    "        newRow = spark.createDataFrame([[1, 0, 0]])\n",
    "        output = output.union(newRow)\n",
    "    \n",
    "    # Creating accuracy evaluator\n",
    "    eval_accuracy = MulticlassClassificationEvaluator(labelCol=\"above_threshold\",\\\n",
    "                                                      predictionCol=\"prediction\", metricName=\"accuracy\",\\\n",
    "                                                      metricLabel=1)\n",
    "    \n",
    "    # Creating precision evaluator\n",
    "    eval_precision = MulticlassClassificationEvaluator(labelCol=\"above_threshold\",\\\n",
    "                                                      predictionCol=\"prediction\", metricName=\"precisionByLabel\",\\\n",
    "                                                      metricLabel=1)\n",
    "    \n",
    "    # Creating recall evaluator\n",
    "    eval_recall = MulticlassClassificationEvaluator(labelCol=\"above_threshold\",\\\n",
    "                                                      predictionCol=\"prediction\", metricName=\"recallByLabel\",\\\n",
    "                                                       metricLabel=1)\n",
    "    \n",
    "    # Getting accuracy, precision and recall using the evaluators\n",
    "    accuracy = eval_accuracy.evaluate(predictions)\n",
    "    precision = eval_precision.evaluate(predictions)\n",
    "    recall = eval_recall.evaluate(predictions)\n",
    "    \n",
    "    # Returing the results\n",
    "    return output, accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "+---------------+----------+-----+\n",
      "|above_threshold|prediction|count|\n",
      "+---------------+----------+-----+\n",
      "|              1|       0.0|  696|\n",
      "|              0|       0.0|55910|\n",
      "|              1|       1.0|  240|\n",
      "|              0|       1.0|  258|\n",
      "+---------------+----------+-----+\n",
      "\n",
      "Accuracy: 0.983293639674979\n",
      "Precision: 0.4819277108433735\n",
      "Recall: 0.2564102564102564\n"
     ]
    }
   ],
   "source": [
    "# Printing metrics for logistic regression\n",
    "output, accuracy, precision, recall = print_metrics(predictionsLR)\n",
    "\n",
    "print(\"Logistic Regression:\")\n",
    "output.show()\n",
    "print(\"Accuracy: \" + str(accuracy))\n",
    "print(\"Precision: \" + str(precision))\n",
    "print(\"Recall: \" + str(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree:\n",
      "+---------------+----------+-----+\n",
      "|above_threshold|prediction|count|\n",
      "+---------------+----------+-----+\n",
      "|              1|       0.0|  890|\n",
      "|              0|       0.0|56058|\n",
      "|              1|       1.0|   46|\n",
      "|              0|       1.0|  110|\n",
      "+---------------+----------+-----+\n",
      "\n",
      "Accuracy: 0.9824880919024936\n",
      "Precision: 0.2948717948717949\n",
      "Recall: 0.049145299145299144\n"
     ]
    }
   ],
   "source": [
    "# Printing metrics for decision tree\n",
    "output, accuracy, precision, recall = print_metrics(predictionsDT)\n",
    "\n",
    "print(\"Decision Tree:\")\n",
    "output.show()\n",
    "print(\"Accuracy: \" + str(accuracy))\n",
    "print(\"Precision: \" + str(precision))\n",
    "print(\"Recall: \" + str(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest:\n",
      "+---------------+----------+-----+\n",
      "|above_threshold|prediction|count|\n",
      "+---------------+----------+-----+\n",
      "|              1|       0.0|  936|\n",
      "|              0|       0.0|56168|\n",
      "|              1|       1.0|    0|\n",
      "|              1|       0.0|    0|\n",
      "+---------------+----------+-----+\n",
      "\n",
      "Accuracy: 0.9836088540207341\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Printing metrics for random forest\n",
    "output, accuracy, precision, recall = print_metrics(predictionsRF)\n",
    "\n",
    "print(\"Random Forest:\")\n",
    "output.show()\n",
    "print(\"Accuracy: \" + str(accuracy))\n",
    "print(\"Precision: \" + str(precision))\n",
    "print(\"Recall: \" + str(recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to accuracy, <b>Random Forest</b> was the most accurate model, followed by <b>Logistic Regression</b> and <b>Decision Tree</b>. However, the three models have very similar accuracy with very minute difference and hence we move on to the other metrics. When it comes to precision and recall, Logistic Regression is the best model followed by Decision Tree and Random Forest. Hence, the ranking would be as follows:\n",
    "    \n",
    "1. Logistic Regression\n",
    "2. Decision Tree\n",
    "3. Random Forest\n",
    "\n",
    "Hence, Logistic Regression will be persisted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Logistic Regression pipeline\n",
    "modelLR.save('best_above_threshold_pedestrian_count_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3.3 Visualising Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_fad81dbdaccd, depth=3, numNodes=13, numClasses=2, numFeatures=190\n",
      "  If (feature 169 in {1.0})\n",
      "   If (feature 104 in {1.0})\n",
      "    If (feature 98 in {1.0})\n",
      "     Predict: 0.0\n",
      "    Else (feature 98 not in {1.0})\n",
      "     Predict: 1.0\n",
      "   Else (feature 104 not in {1.0})\n",
      "    If (feature 105 in {1.0})\n",
      "     Predict: 1.0\n",
      "    Else (feature 105 not in {1.0})\n",
      "     Predict: 0.0\n",
      "  Else (feature 169 not in {1.0})\n",
      "   If (feature 166 in {1.0})\n",
      "    If (feature 105 in {1.0})\n",
      "     Predict: 1.0\n",
      "    Else (feature 105 not in {1.0})\n",
      "     Predict: 0.0\n",
      "   Else (feature 166 not in {1.0})\n",
      "    Predict: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualisng the decision tree decision making at the node level\n",
    "model_dt = modelDT.stages[-1]\n",
    "print(model_dt.toDebugString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference: https://www.timlrx.com/blog/feature-selection-using-feature-importance-score-creating-a-pyspark-estimator\n",
    "\n",
    "# Function to extract features, correspond them to the column and sorting them according to score\n",
    "def extract_features(featureImportance, df, featuresCol):\n",
    "    extract = []\n",
    "    for i in df.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"]:\n",
    "        extract = extract + df.schema[featuresCol].metadata[\"ml_attr\"][\"attrs\"][i]\n",
    "    variableList = pd.DataFrame(extract)\n",
    "    variableList['score'] = variableList['idx'].apply(lambda x: featureImportance[x])\n",
    "    return(variableList.sort_values('score', ascending = False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>name</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>169</td>\n",
       "      <td>Sensor_ID_vec_38</td>\n",
       "      <td>0.499630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>105</td>\n",
       "      <td>day_of_week_vec_6</td>\n",
       "      <td>0.231910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>166</td>\n",
       "      <td>Sensor_ID_vec_35</td>\n",
       "      <td>0.152003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     idx               name     score\n",
       "169  169   Sensor_ID_vec_38  0.499630\n",
       "105  105  day_of_week_vec_6  0.231910\n",
       "166  166   Sensor_ID_vec_35  0.152003"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Printing top 3 features according to the score\n",
    "extract_features(model_dt.featureImportances, predictionsDT, \"features\").head(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
